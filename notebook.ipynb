{"cells": [{"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='d083a218-a227-46c1-b8da-bc1aeb4e0612', project_access_token='p-86a8b18f435d909c61de3fdc2206bce722713ed5')\npc = project.project_context\n", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install scikit-learn --upgrade\n!pip install scipy --upgrade\n!pip install imblearn --upgrade\n!pip install xgboost --upgrade", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "Requirement already up-to-date: scikit-learn in /opt/conda/envs/Python36/lib/python3.6/site-packages (0.23.2)\nRequirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn) (2.1.0)\nRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn) (0.16.0)\nRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn) (1.15.4)\nRequirement already up-to-date: scipy in /opt/conda/envs/Python36/lib/python3.6/site-packages (1.5.2)\nRequirement already satisfied, skipping upgrade: numpy>=1.14.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scipy) (1.15.4)\nRequirement already up-to-date: imblearn in /opt/conda/envs/Python36/lib/python3.6/site-packages (0.0)\nRequirement already satisfied, skipping upgrade: imbalanced-learn in /opt/conda/envs/Python36/lib/python3.6/site-packages (from imblearn) (0.7.0)\nRequirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (0.16.0)\nRequirement already satisfied, skipping upgrade: scikit-learn>=0.23 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (0.23.2)\nRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (1.15.4)\nRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (1.5.2)\nRequirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-learn>=0.23->imbalanced-learn->imblearn) (2.1.0)\nRequirement already up-to-date: xgboost in /opt/conda/envs/Python36/lib/python3.6/site-packages (1.2.0)\nRequirement already satisfied, skipping upgrade: scipy in /opt/conda/envs/Python36/lib/python3.6/site-packages (from xgboost) (1.5.2)\nRequirement already satisfied, skipping upgrade: numpy in /opt/conda/envs/Python36/lib/python3.6/site-packages (from xgboost) (1.15.4)\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold, cross_validate\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.over_sampling import SMOTE\nfrom itertools import combinations\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import linear_model\nimport seaborn as sns\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.svm import SVR\nfrom scipy import stats\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Linear Regression"}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/654986294958/master/train_dataset_digitalhouse.csv\ndf_training_dataset = pd.read_csv(r'train_dataset_digitalhouse.csv')\ndf_training_dataset=pd.get_dummies(df_training_dataset, columns=['GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH'])\n\nfeatures=['EDAD', \n          'AVG_DH',\n          'MINUTES_DH', \n          'EXPERIENCIA',\n       'GENERO_FEMENINO', 'GENERO_MASCULINO', \n        'RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL', 'RESIDENCIA_MEXICO', \n        'NV_ESTUDIO_POST_GRADUATE','NV_ESTUDIO_TERTIARY', 'NV_ESTUDIO_UNIVERSITARY',\n       'ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL',\n       'ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING',\n       'ESTUDIO_PREV_MARKETING', \n         #'TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION'\n         ]\ntarget='DIAS_EMP'\n\n\nX=df_training_dataset.copy()[features]\ny=df_training_dataset.copy()[target]\n\n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    X.loc[X[i].isnull(),[i]]=X[i].mean()\n\n#Detectando outliers\n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    Q1 = X[i].quantile(0.25)\n#    Q3 = X[i].quantile(0.75)\n#    IQR = Q3 - Q1\n#    X.loc[(X[i] < (Q1 - 1.5 * IQR)),[i]]=Q1 - 1.5 * IQR\n    #X.loc[(X[i] < (Q1 - 1.5 * IQR)),[i]]=X.loc[(X[i] < (Q1 - 1.5 * IQR)),[i]]/X[i].std()\n#    X.loc[(X[i] > (Q3 + 1.5 * IQR)),[i]]=Q3 + 1.5 * IQR\n    #X.loc[(X[i] < (Q1 - 1.5 * IQR)),[i]]=X.loc[(X[i] < (Q1 - 1.5 * IQR)),[i]]/X[i].std()\n    \nimp = IterativeImputer(max_iter=25)\nimp.fit(X)\nX=pd.DataFrame(imp.transform(X),columns=X.columns)\n\n#from sklearn.impute import KNNImputer\n#imp=KNNImputer(n_neighbors=15, weights=\"uniform\")\n#imp.fit(X)\n#X=pd.DataFrame(imp.transform(X),columns=X.columns) \n\n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    X.loc[X[i].isnull(),[i]]=X[i].median()\n\nfor i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n    X[i]=(X[i]-X[i].mean())/X[i].std()\n\n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    X[i]=(X[i]-X[i].min(axis=0))/(X[i].max(axis=0)-X[i].min(axis=0))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "probar=['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA','GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH']\ncombinaciones=[]\nfor i in range(10):\n    for j in combinations(probar,i):\n        if len(list(j))>0:\n            combinaciones.append(list(j))\n            \ngenero=['GENERO_MASCULINO','GENERO_FEMENINO']\nresidencia=['RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL','RESIDENCIA_MEXICO']\nnv_estudio=['NV_ESTUDIO_POST_GRADUATE','NV_ESTUDIO_TERTIARY','NV_ESTUDIO_UNIVERSITARY']\nestudio_prev=['ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL','ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING','ESTUDIO_PREV_MARKETING']\ntrack_dh=['TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION']\n            \nprobar=[]\nfor i in combinaciones:\n    lista=i\n    if 'GENERO' in i:\n        lista.remove('GENERO')\n        lista.extend(genero)\n    if 'RESIDENCIA' in i:\n        lista.remove('RESIDENCIA')\n        lista.extend(residencia)\n    if 'NV_ESTUDIO' in i:\n        lista.remove('NV_ESTUDIO')\n        lista.extend(nv_estudio)\n    if 'ESTUDIO_PREV' in i:\n        lista.remove('ESTUDIO_PREV')\n        lista.extend(estudio_prev)\n    if 'TRACK_DH' in i:\n        lista.remove('TRACK_DH')\n        lista.extend(track_dh)\n    probar.append(lista)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#promedios=[]\n#for grupo in probar:\n#    X=df_training_dataset[grupo]\n#    y=df_training_dataset['DIAS_EMP']\n#    poly = PolynomialFeatures(2)\n#    X=poly.fit_transform(X)\n#    lr= LinearRegression()\n#    cv=cross_validate(lr,X,y,cv=10,scoring='r2')\n#    promedios.append([grupo,cv['test_score'].mean()])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#pd.DataFrame(promedios).sort_values(by=1,ascending=False).head().loc[486,:]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "poly = PolynomialFeatures(2)\nX=poly.fit_transform(X)\nlr= LinearRegression()\ncv=cross_validate(lr,X,y,cv=10,scoring='r2')\nprint(cv['test_score'].mean())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# KNN"}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/654986294958/master/train_dataset_digitalhouse.csv\ndf_training_dataset = pd.read_csv(r'train_dataset_digitalhouse.csv')\ndf_training_dataset=pd.get_dummies(df_training_dataset, columns=['GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH'])\n\n#imputar\n\nfor i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n    df_training_dataset.loc[df_training_dataset[i].isnull(),[i]]=df_training_dataset[i].mean()\n    \n#from sklearn.impute import KNNImputer\n#imp=KNNImputer(n_neighbors=30, weights=\"uniform\")\n#imp.fit(df_training_dataset)\n#df_training_dataset=pd.DataFrame(imp.transform(df_training_dataset),columns=df_training_dataset.columns)    \n\n#from sklearn.experimental import enable_iterative_imputer\n#from sklearn.impute import IterativeImputer\n#imp = IterativeImputer(max_iter=30, random_state=0)\n#imp.fit(df_training_dataset)\n#df_training_dataset=pd.DataFrame(imp.transform(df_training_dataset),columns=df_training_dataset.columns) \n\n\n#escalar\n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset[i]=(df_training_dataset[i]-df_training_dataset[i].mean())/df_training_dataset[i].std()\n    \n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset[i]=(df_training_dataset[i]-df_training_dataset[i].min(axis=0))/(df_training_dataset[i].max(axis=0)-df_training_dataset[i].min(axis=0))\n\n\nfeatures=['EDAD', \n          #'AVG_DH',\n          'MINUTES_DH', 'EXPERIENCIA',\n       'GENERO_FEMENINO', 'GENERO_MASCULINO', 'RESIDENCIA_ARGENTINA',\n       #'RESIDENCIA_BRAZIL', 'RESIDENCIA_MEXICO', 'NV_ESTUDIO_POST_GRADUATE',\n       'NV_ESTUDIO_TERTIARY', 'NV_ESTUDIO_UNIVERSITARY',\n       'ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL',\n       'ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING',\n       'ESTUDIO_PREV_MARKETING', \n        #'TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION'\n         ]\ntarget='DIAS_EMP'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "probar=['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA','GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH']\ncombinaciones=[]\nfor i in range(10):\n    for j in combinations(probar,i):\n        if len(list(j))>0:\n            combinaciones.append(list(j))\n            \ngenero=['GENERO_MASCULINO','GENERO_FEMENINO']\nresidencia=['RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL','RESIDENCIA_MEXICO']\nnv_estudio=['NV_ESTUDIO_POST_GRADUATE','NV_ESTUDIO_TERTIARY','NV_ESTUDIO_UNIVERSITARY']\nestudio_prev=['ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL','ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING','ESTUDIO_PREV_MARKETING']\ntrack_dh=['TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION']\n            \nprobar=[]\nfor i in combinaciones:\n    lista=i\n    if 'GENERO' in i:\n        lista.remove('GENERO')\n        lista.extend(genero)\n    if 'RESIDENCIA' in i:\n        lista.remove('RESIDENCIA')\n        lista.extend(residencia)\n    if 'NV_ESTUDIO' in i:\n        lista.remove('NV_ESTUDIO')\n        lista.extend(nv_estudio)\n    if 'ESTUDIO_PREV' in i:\n        lista.remove('ESTUDIO_PREV')\n        lista.extend(estudio_prev)\n    if 'TRACK_DH' in i:\n        lista.remove('TRACK_DH')\n        lista.extend(track_dh)\n    probar.append(lista)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#promedios=[]\n#hola=0\n#for grupo in probar:\n#    X=df_training_dataset[grupo]\n#    y=df_training_dataset['DIAS_EMP']\n#    knn= KNeighborsRegressor(n_neighbors=10)\n#    cv=cross_validate(knn,X,y,cv=10,scoring='r2')\n#    promedios.append([grupo,cv['test_score'].mean()])\n#    print(hola)\n#    hola=hola+1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#pd.DataFrame(promedios).sort_values(by=1,ascending=False).head().loc[419,0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#promedios=[]\n#for i in range(1,30):\n#    X=df_training_dataset[features]\n#    y=df_training_dataset[target]\n#    knn= KNeighborsRegressor(n_neighbors=i)\n#    cv=cross_validate(knn,X,y,cv=10,scoring='r2')\n#    promedios.append([i,cv['test_score'].mean()])\n#pd.DataFrame(promedios).sort_values(by=1,ascending=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X=df_training_dataset[features]\ny=df_training_dataset[target]\nknn= KNeighborsRegressor(n_neighbors=19)\ncv=cross_validate(knn,X,y,cv=10,scoring='r2')\nprint(cv['test_score'].mean())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# DecisionTree"}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/654986294958/master/train_dataset_digitalhouse.csv\ndf_training_dataset = pd.read_csv(r'train_dataset_digitalhouse.csv')\ndf_training_dataset=pd.get_dummies(df_training_dataset, columns=['GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH'])\n\n#imputar\n\n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset.loc[df_training_dataset[i].isnull(),[i]]=df_training_dataset[i].median()\n    \n#from sklearn.impute import KNNImputer\n#imp=KNNImputer(n_neighbors=30, weights=\"uniform\")\n#imp.fit(df_training_dataset)\n#df_training_dataset=pd.DataFrame(imp.transform(df_training_dataset),columns=df_training_dataset.columns)    \n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=18, random_state=0)\nimp.fit(df_training_dataset)\ndf_training_dataset=pd.DataFrame(imp.transform(df_training_dataset),columns=df_training_dataset.columns) \n\n\n#escalar\n\n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset[i]=(df_training_dataset[i]-df_training_dataset[i].mean())/df_training_dataset[i].std()\n    \n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset[i]=(df_training_dataset[i]-df_training_dataset[i].min(axis=0))/(df_training_dataset[i].max(axis=0)-df_training_dataset[i].min(axis=0))\n\n\nfeatures=['EDAD', \n          #'AVG_DH',\n          #'MINUTES_DH', \n          #'EXPERIENCIA',\n       'GENERO_FEMENINO', 'GENERO_MASCULINO', \n    #'RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL', 'RESIDENCIA_MEXICO', 'NV_ESTUDIO_POST_GRADUATE',\n       'NV_ESTUDIO_TERTIARY', 'NV_ESTUDIO_UNIVERSITARY',\n       'ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL',\n       'ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING',\n       'ESTUDIO_PREV_MARKETING', \n         #'TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION'\n         ]\ntarget='DIAS_EMP'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "probar=['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA','GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH']\ncombinaciones=[]\nfor i in range(10):\n    for j in combinations(probar,i):\n        if len(list(j))>0:\n            combinaciones.append(list(j))\n            \ngenero=['GENERO_MASCULINO','GENERO_FEMENINO']\nresidencia=['RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL','RESIDENCIA_MEXICO']\nnv_estudio=['NV_ESTUDIO_POST_GRADUATE','NV_ESTUDIO_TERTIARY','NV_ESTUDIO_UNIVERSITARY']\nestudio_prev=['ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL','ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING','ESTUDIO_PREV_MARKETING']\ntrack_dh=['TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION']\n            \nprobar=[]\nfor i in combinaciones:\n    lista=i\n    if 'GENERO' in i:\n        lista.remove('GENERO')\n        lista.extend(genero)\n    if 'RESIDENCIA' in i:\n        lista.remove('RESIDENCIA')\n        lista.extend(residencia)\n    if 'NV_ESTUDIO' in i:\n        lista.remove('NV_ESTUDIO')\n        lista.extend(nv_estudio)\n    if 'ESTUDIO_PREV' in i:\n        lista.remove('ESTUDIO_PREV')\n        lista.extend(estudio_prev)\n    if 'TRACK_DH' in i:\n        lista.remove('TRACK_DH')\n        lista.extend(track_dh)\n    probar.append(lista)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "promedios=[]\nhola=0\nfor grupo in probar:\n    X=df_training_dataset[grupo]\n    y=df_training_dataset[target]\n    dtr= DecisionTreeRegressor(max_depth=8)\n    cv=cross_validate(dtr,X,y,cv=10,scoring='r2')\n    promedios.append([grupo,cv['test_score'].mean()])\n    #print(hola)\n    #hola=hola+1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pd.DataFrame(promedios).sort_values(by=1,ascending=False).loc[178,0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#X=df_training_dataset[features]\n#y=df_training_dataset[target]\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n#dtr= DecisionTreeRegressor()\n#values={'max_depth':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28],'max_features':[1,2,3,4,5,6,7,8,9,10]}\n#grid=GridSearchCV(dtr, param_grid = values,scoring = 'r2')\n#grid.fit(X_train,y_train)\n#print(grid.best_params_)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X=df_training_dataset[features]\ny=df_training_dataset[target]\ndtr= DecisionTreeRegressor(max_depth=7,max_features=8)\ncv=cross_validate(dtr,X,y,cv=10,scoring='r2')\nprint(cv['test_score'].mean())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Lasso"}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/654986294958/master/train_dataset_digitalhouse.csv\ndf_training_dataset = pd.read_csv(r'train_dataset_digitalhouse.csv')\ndf_training_dataset=pd.get_dummies(df_training_dataset, columns=['GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH'])\n\n#imputar\n\n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset.loc[df_training_dataset[i].isnull(),[i]]=df_training_dataset[i].median()\n    \n#from sklearn.impute import KNNImputer\n#imp=KNNImputer(n_neighbors=5, weights=\"uniform\")\n#imp.fit(df_training_dataset)\n#df_training_dataset=pd.DataFrame(imp.transform(df_training_dataset),columns=df_training_dataset.columns)    \n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=18, random_state=0)\nimp.fit(df_training_dataset)\ndf_training_dataset=pd.DataFrame(imp.transform(df_training_dataset),columns=df_training_dataset.columns) \n\n\n#escalar\n\n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset[i]=(df_training_dataset[i]-df_training_dataset[i].mean())/df_training_dataset[i].std()\n    \n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset[i]=(df_training_dataset[i]-df_training_dataset[i].min(axis=0))/(df_training_dataset[i].max(axis=0)-df_training_dataset[i].min(axis=0))\n\n\nfeatures=['EDAD', \n          'AVG_DH',\n          'MINUTES_DH', \n          #'EXPERIENCIA',\n       'GENERO_FEMENINO', 'GENERO_MASCULINO', \n    #'RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL', 'RESIDENCIA_MEXICO', \n    'NV_ESTUDIO_POST_GRADUATE','NV_ESTUDIO_TERTIARY', 'NV_ESTUDIO_UNIVERSITARY',\n       'ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL',\n       'ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING',\n       'ESTUDIO_PREV_MARKETING', \n         #'TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION'\n         ]\ntarget='DIAS_EMP'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "probar=['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA','GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH']\ncombinaciones=[]\nfor i in range(10):\n    for j in combinations(probar,i):\n        if len(list(j))>0:\n            combinaciones.append(list(j))\n            \ngenero=['GENERO_MASCULINO','GENERO_FEMENINO']\nresidencia=['RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL','RESIDENCIA_MEXICO']\nnv_estudio=['NV_ESTUDIO_POST_GRADUATE','NV_ESTUDIO_TERTIARY','NV_ESTUDIO_UNIVERSITARY']\nestudio_prev=['ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL','ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING','ESTUDIO_PREV_MARKETING']\ntrack_dh=['TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION']\n            \nprobar=[]\nfor i in combinaciones:\n    lista=i\n    if 'GENERO' in i:\n        lista.remove('GENERO')\n        lista.extend(genero)\n    if 'RESIDENCIA' in i:\n        lista.remove('RESIDENCIA')\n        lista.extend(residencia)\n    if 'NV_ESTUDIO' in i:\n        lista.remove('NV_ESTUDIO')\n        lista.extend(nv_estudio)\n    if 'ESTUDIO_PREV' in i:\n        lista.remove('ESTUDIO_PREV')\n        lista.extend(estudio_prev)\n    if 'TRACK_DH' in i:\n        lista.remove('TRACK_DH')\n        lista.extend(track_dh)\n    probar.append(lista)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "promedios=[]\nhola=0\nfor grupo in probar:\n    X=df_training_dataset[grupo]\n    y=df_training_dataset[target]\n    lasso= linear_model.Lasso(alpha=0.01)\n    cv=cross_validate(lasso,X,y,cv=10,scoring='r2')\n    promedios.append([grupo,cv['test_score'].mean()])\n    #print(hola)\n    #hola=hola+1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pd.DataFrame(promedios).sort_values(by=1,ascending=False).loc[394,0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X=df_training_dataset[features]\ny=df_training_dataset[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\nlasso= linear_model.Lasso()\nvalues={'alpha':[0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.5,1.8,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,20,30,50,100,1000,2000,10000]}\ngrid=GridSearchCV(lasso, param_grid = values,scoring = 'r2')\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X=df_training_dataset[features]\ny=df_training_dataset[target]\nlasso= linear_model.Lasso(alpha=0.01)\ncv=cross_validate(lasso,X,y,cv=10,scoring='r2')\nprint([cv['test_score'].mean()])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Ridge"}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/654986294958/master/train_dataset_digitalhouse.csv\ndf_training_dataset = pd.read_csv(r'train_dataset_digitalhouse.csv')\ndf_training_dataset=pd.get_dummies(df_training_dataset, columns=['GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH'])\n\n#imputar\n\n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset.loc[df_training_dataset[i].isnull(),[i]]=df_training_dataset[i].median()\n    \n#from sklearn.impute import KNNImputer\n#imp=KNNImputer(n_neighbors=5, weights=\"uniform\")\n#imp.fit(df_training_dataset)\n#df_training_dataset=pd.DataFrame(imp.transform(df_training_dataset),columns=df_training_dataset.columns)    \n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=15, random_state=0)\nimp.fit(df_training_dataset)\ndf_training_dataset=pd.DataFrame(imp.transform(df_training_dataset),columns=df_training_dataset.columns) \n\n\n#escalar\n\n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset[i]=(df_training_dataset[i]-df_training_dataset[i].mean())/df_training_dataset[i].std()\n    \n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset[i]=(df_training_dataset[i]-df_training_dataset[i].min(axis=0))/(df_training_dataset[i].max(axis=0)-df_training_dataset[i].min(axis=0))\n\n\nfeatures=['EDAD', \n          'AVG_DH',\n          'MINUTES_DH', \n          'EXPERIENCIA',\n       'GENERO_FEMENINO', 'GENERO_MASCULINO', \n    'RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL', 'RESIDENCIA_MEXICO', \n    'NV_ESTUDIO_POST_GRADUATE','NV_ESTUDIO_TERTIARY', 'NV_ESTUDIO_UNIVERSITARY',\n       'ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL',\n       'ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING',\n       'ESTUDIO_PREV_MARKETING', \n         #'TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION'\n         ]\ntarget='DIAS_EMP'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "probar=['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA','GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH']\ncombinaciones=[]\nfor i in range(10):\n    for j in combinations(probar,i):\n        if len(list(j))>0:\n            combinaciones.append(list(j))\n            \ngenero=['GENERO_MASCULINO','GENERO_FEMENINO']\nresidencia=['RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL','RESIDENCIA_MEXICO']\nnv_estudio=['NV_ESTUDIO_POST_GRADUATE','NV_ESTUDIO_TERTIARY','NV_ESTUDIO_UNIVERSITARY']\nestudio_prev=['ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL','ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING','ESTUDIO_PREV_MARKETING']\ntrack_dh=['TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION']\n            \nprobar=[]\nfor i in combinaciones:\n    lista=i\n    if 'GENERO' in i:\n        lista.remove('GENERO')\n        lista.extend(genero)\n    if 'RESIDENCIA' in i:\n        lista.remove('RESIDENCIA')\n        lista.extend(residencia)\n    if 'NV_ESTUDIO' in i:\n        lista.remove('NV_ESTUDIO')\n        lista.extend(nv_estudio)\n    if 'ESTUDIO_PREV' in i:\n        lista.remove('ESTUDIO_PREV')\n        lista.extend(estudio_prev)\n    if 'TRACK_DH' in i:\n        lista.remove('TRACK_DH')\n        lista.extend(track_dh)\n    probar.append(lista)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "promedios=[]\nhola=0\nfor grupo in probar:\n    X=df_training_dataset[grupo]\n    y=df_training_dataset[target]\n    ridge= linear_model.Ridge(alpha=0.7)\n    cv=cross_validate(ridge,X,y,cv=10,scoring='r2')\n    promedios.append([grupo,cv['test_score'].mean()])\n    #print(hola)\n    #hola=hola+1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pd.DataFrame(promedios).sort_values(by=1,ascending=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X=df_training_dataset[features]\ny=df_training_dataset[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\nridge= linear_model.Ridge()\nvalues={'alpha':[0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.5,1.8,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,20,30,50,100,1000,2000,10000]}\ngrid=GridSearchCV(ridge, param_grid = values,scoring = 'r2')\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X=df_training_dataset[features]\ny=df_training_dataset[target]\nridge= linear_model.Ridge(alpha=0.7)\ncv=cross_validate(ridge,X,y,cv=10,scoring='r2')\nprint([cv['test_score'].mean()])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# XGBoost"}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/654986294958/master/train_dataset_digitalhouse.csv\ndf_training_dataset = pd.read_csv(r'train_dataset_digitalhouse.csv')\ndf_training_dataset=pd.get_dummies(df_training_dataset, columns=['GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/654986294958/master/train_dataset_digitalhouse.csv\ndf_training_dataset = pd.read_csv(r'train_dataset_digitalhouse.csv')\ndf_training_dataset=pd.get_dummies(df_training_dataset, columns=['GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH'])\n\n#imputar\n\nfeatures=['EDAD', \n          'AVG_DH',\n          'MINUTES_DH', \n          'EXPERIENCIA',\n       'GENERO_FEMENINO', 'GENERO_MASCULINO', \n    'RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL', 'RESIDENCIA_MEXICO', \n    'NV_ESTUDIO_POST_GRADUATE','NV_ESTUDIO_TERTIARY', 'NV_ESTUDIO_UNIVERSITARY',\n       'ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL',\n       'ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING',\n       'ESTUDIO_PREV_MARKETING', \n         #'TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION'\n         ]\ntarget='DIAS_EMP'\n\nX=df_training_dataset.copy()[features]\ny=df_training_dataset.copy()[target]\n\n#for i in ['EDAD',\n#          #'AVG_DH','MINUTES_DH','EXPERIENCIA'\n#         ]:\n#    X.loc[X[i].isnull(),[i]]=X[i].mean()\n    \n#from sklearn.impute import KNNImputer\n#imp=KNNImputer(n_neighbors=10, weights=\"uniform\")\n#imp.fit(df_training_dataset)\n#df_training_dataset=pd.DataFrame(imp.transform(df_training_dataset),columns=df_training_dataset.columns) \n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=15, random_state=0)\nimp.fit(X)\nX=pd.DataFrame(imp.transform(X),columns=X.columns)\n\n#iso = IsolationForest(contamination=0.3)\n#yhat = iso.fit_predict(X)\n#X.loc[yhat==-1,['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']]=np.nan\n\n\n#from sklearn.experimental import enable_iterative_imputer\n#from sklearn.impute import IterativeImputer\n#imp = IterativeImputer(max_iter=15, random_state=0)\n#imp.fit(X)\n#X=pd.DataFrame(imp.transform(X),columns=X.columns)\n\n#escalar\n\n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset[i]=(df_training_dataset[i]-df_training_dataset[i].mean())/df_training_dataset[i].std()\n    \n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset[i]=(df_training_dataset[i]-df_training_dataset[i].min(axis=0))/(df_training_dataset[i].max(axis=0)-df_training_dataset[i].min(axis=0))\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_training_dataset.loc[yhat==-1,['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']]=np.nan", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "probar=['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA','GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH']\ncombinaciones=[]\nfor i in range(10):\n    for j in combinations(probar,i):\n        if len(list(j))>0:\n            combinaciones.append(list(j))\n            \ngenero=['GENERO_MASCULINO','GENERO_FEMENINO']\nresidencia=['RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL','RESIDENCIA_MEXICO']\nnv_estudio=['NV_ESTUDIO_POST_GRADUATE','NV_ESTUDIO_TERTIARY','NV_ESTUDIO_UNIVERSITARY']\nestudio_prev=['ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL','ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING','ESTUDIO_PREV_MARKETING']\ntrack_dh=['TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION']\n            \nprobar=[]\nfor i in combinaciones:\n    lista=i\n    if 'GENERO' in i:\n        lista.remove('GENERO')\n        lista.extend(genero)\n    if 'RESIDENCIA' in i:\n        lista.remove('RESIDENCIA')\n        lista.extend(residencia)\n    if 'NV_ESTUDIO' in i:\n        lista.remove('NV_ESTUDIO')\n        lista.extend(nv_estudio)\n    if 'ESTUDIO_PREV' in i:\n        lista.remove('ESTUDIO_PREV')\n        lista.extend(estudio_prev)\n    if 'TRACK_DH' in i:\n        lista.remove('TRACK_DH')\n        lista.extend(track_dh)\n    probar.append(lista)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "promedios=[]\nhola=0\nfor grupo in probar:\n    X=df_training_dataset[grupo]\n    y=df_training_dataset[target]\n    xgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', learning_rate = 0.09,max_depth = 5, n_estimators = 120,n_jobs=-1)\n    cv=cross_validate(xgb_reg,X,y,cv=3,scoring='r2')\n    promedios.append([grupo,cv['test_score'].mean()])\n    print(hola)\n    hola=hola+1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pd.DataFrame(promedios).sort_values(by=1,ascending=False).loc[504,0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X=df_training_dataset[features]\ny=df_training_dataset[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\nxgb_reg = xgb.XGBRegressor(n_jobs=-1,learning_rate=0.09,max_depth=5)\nvalues={'learning_rate':[0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n       'max_depth':[5,6,7,8,9,10,11,12,13,14,15]}\ngrid=GridSearchCV(xgb_reg, param_grid = values,scoring = 'r2')\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#X=df_training_dataset[features]\n#y=df_training_dataset[target]\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nxgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', learning_rate = 0.12,max_depth = 5, n_estimators = 120,n_jobs=-1)\ncv=cross_validate(xgb_reg,X,y,cv=10,scoring='r2')\nprint([cv['test_score'].mean()])\nprint(cv['test_score'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Gradient Boosting"}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/654986294958/master/train_dataset_digitalhouse.csv\ndf_training_dataset = pd.read_csv(r'train_dataset_digitalhouse.csv')\ndf_training_dataset=pd.get_dummies(df_training_dataset, columns=['GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH'])\n\nfeatures=['EDAD', \n          'AVG_DH',\n          'MINUTES_DH', \n          #'EXPERIENCIA',\n        'GENERO_FEMENINO', 'GENERO_MASCULINO', \n        'RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL', 'RESIDENCIA_MEXICO', \n        'NV_ESTUDIO_POST_GRADUATE','NV_ESTUDIO_TERTIARY', 'NV_ESTUDIO_UNIVERSITARY',\n       'ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL',\n       'ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING',\n       'ESTUDIO_PREV_MARKETING', \n         #'TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION'\n         ]\ntarget='DIAS_EMP'\n\n\nX=df_training_dataset.copy()[features]\ny=df_training_dataset.copy()[target]\n\n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    X.loc[X[i].isnull(),[i]]=X[i].mean()\n    \n#from sklearn.impute import KNNImputer\n#imp=KNNImputer(n_neighbors=10, weights=\"uniform\")\n#imp.fit(df_training_dataset)\n#df_training_dataset=pd.DataFrame(imp.transform(df_training_dataset),columns=df_training_dataset.columns) \n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=15, random_state=0)\nimp.fit(X)\nX=pd.DataFrame(imp.transform(X),columns=X.columns)\n\n#iso = IsolationForest(contamination=0.3)\n#yhat = iso.fit_predict(X)\n#X.loc[yhat==-1,['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']]=np.nan\n\n\n#from sklearn.experimental import enable_iterative_imputer\n#from sklearn.impute import IterativeImputer\n#imp = IterativeImputer(max_iter=15, random_state=0)\n#imp.fit(X)\n#X=pd.DataFrame(imp.transform(X),columns=X.columns)\n\n#escalar\n\n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset[i]=(df_training_dataset[i]-df_training_dataset[i].mean())/df_training_dataset[i].std()\n    \n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset[i]=(df_training_dataset[i]-df_training_dataset[i].min(axis=0))/(df_training_dataset[i].max(axis=0)-df_training_dataset[i].min(axis=0))\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "probar=['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA','GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH']\ncombinaciones=[]\nfor i in range(10):\n    for j in combinations(probar,i):\n        if len(list(j))>0:\n            combinaciones.append(list(j))\n            \ngenero=['GENERO_MASCULINO','GENERO_FEMENINO']\nresidencia=['RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL','RESIDENCIA_MEXICO']\nnv_estudio=['NV_ESTUDIO_POST_GRADUATE','NV_ESTUDIO_TERTIARY','NV_ESTUDIO_UNIVERSITARY']\nestudio_prev=['ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL','ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING','ESTUDIO_PREV_MARKETING']\ntrack_dh=['TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION']\n            \nprobar=[]\nfor i in combinaciones:\n    lista=i\n    if 'GENERO' in i:\n        lista.remove('GENERO')\n        lista.extend(genero)\n    if 'RESIDENCIA' in i:\n        lista.remove('RESIDENCIA')\n        lista.extend(residencia)\n    if 'NV_ESTUDIO' in i:\n        lista.remove('NV_ESTUDIO')\n        lista.extend(nv_estudio)\n    if 'ESTUDIO_PREV' in i:\n        lista.remove('ESTUDIO_PREV')\n        lista.extend(estudio_prev)\n    if 'TRACK_DH' in i:\n        lista.remove('TRACK_DH')\n        lista.extend(track_dh)\n    probar.append(lista)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "promedios=[]\nhola=0\nfor grupo in probar:\n    X=df_training_dataset[grupo]\n    y=df_training_dataset[target]\n    xgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', learning_rate = 0.09,max_depth = 5, n_estimators = 120,n_jobs=-1)\n    cv=cross_validate(xgb_reg,X,y,cv=3,scoring='r2')\n    promedios.append([grupo,cv['test_score'].mean()])\n    print(hola)\n    hola=hola+1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pd.DataFrame(promedios).sort_values(by=1,ascending=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X=df_training_dataset[features]\ny=df_training_dataset[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\nxgb_reg = xgb.XGBRegressor(n_jobs=-1,learning_rate=0.09,max_depth=5)\nvalues={'learning_rate':[0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n       'max_depth':[5,6,7,8,9,10,11,12,13,14,15]}\ngrid=GridSearchCV(xgb_reg, param_grid = values,scoring = 'r2')\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\ngbreg = GradientBoostingRegressor(learning_rate=0.13,n_estimators=90,max_depth=3,alpha=0.9)\ncv=cross_validate(gbreg,X,y,cv=5,scoring='r2')\nprint([cv['test_score'].mean()])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# SVC"}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/654986294958/master/train_dataset_digitalhouse.csv\ndf_training_dataset = pd.read_csv(r'train_dataset_digitalhouse.csv')\ndf_training_dataset=pd.get_dummies(df_training_dataset, columns=['GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH'])\n\n#imputar\n\nfeatures=['EDAD', \n          'AVG_DH',\n          'MINUTES_DH', \n          'EXPERIENCIA',\n       'GENERO_FEMENINO', 'GENERO_MASCULINO', \n    'RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL', 'RESIDENCIA_MEXICO', \n    'NV_ESTUDIO_POST_GRADUATE','NV_ESTUDIO_TERTIARY', 'NV_ESTUDIO_UNIVERSITARY',\n       'ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL',\n       'ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING',\n       'ESTUDIO_PREV_MARKETING', \n         #'TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION'\n         ]\ntarget='DIAS_EMP'\n\nX=df_training_dataset.copy()[features]\ny=df_training_dataset.copy()[target]\n\nfor i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n    X.loc[X[i].isnull(),[i]]=X[i].mean()\n    \n#from sklearn.impute import KNNImputer\n#imp=KNNImputer(n_neighbors=10, weights=\"uniform\")\n#imp.fit(df_training_dataset)\n#df_training_dataset=pd.DataFrame(imp.transform(df_training_dataset),columns=df_training_dataset.columns) \n\n#from sklearn.experimental import enable_iterative_imputer\n#from sklearn.impute import IterativeImputer\n#imp = IterativeImputer(max_iter=15, random_state=0)\n#imp.fit(X)\n#X=pd.DataFrame(imp.transform(X),columns=X.columns)\n\n#iso = IsolationForest(contamination=0.3)\n#yhat = iso.fit_predict(X)\n#X.loc[yhat==-1,['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']]=np.nan\n\n\n#from sklearn.experimental import enable_iterative_imputer\n#from sklearn.impute import IterativeImputer\n#imp = IterativeImputer(max_iter=15, random_state=0)\n#imp.fit(X)\n#X=pd.DataFrame(imp.transform(X),columns=X.columns)\n\n#escalar\n\nfor i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n    df_training_dataset[i]=(df_training_dataset[i]-df_training_dataset[i].mean())/df_training_dataset[i].std()\n    \n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    df_training_dataset[i]=(df_training_dataset[i]-df_training_dataset[i].min(axis=0))/(df_training_dataset[i].max(axis=0)-df_training_dataset[i].min(axis=0))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nregr = SVR(kernel='rbf')\ncv=cross_validate(regr,X,y,cv=5,scoring='r2')\nprint([cv['test_score'].mean()])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# USANDO MIS ULTIMAS ARMAS"}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/654986294958/master/train_dataset_digitalhouse.csv\ndf_training_dataset = pd.read_csv(r'train_dataset_digitalhouse.csv')\ndf_training_dataset=pd.get_dummies(df_training_dataset, columns=['GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH'])\n\nfeatures=['EDAD', \n          'AVG_DH',\n          'MINUTES_DH', \n          'EXPERIENCIA',\n           'GENERO_FEMENINO', \n          'GENERO_MASCULINO', \n            'RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL', 'RESIDENCIA_MEXICO', \n            'NV_ESTUDIO_POST_GRADUATE','NV_ESTUDIO_TERTIARY', 'NV_ESTUDIO_UNIVERSITARY',\n            'ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL',\n           'ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING',\n           'ESTUDIO_PREV_MARKETING', \n             #'TRACK_DH_DATA', 'TRACK_DH_EJECUTIVO','TRACK_DH_MARKETING', 'TRACK_DH_PROGRAMACION'\n         ]\ntarget='DIAS_EMP'\n\n\nX=df_training_dataset.copy()[features]\ny=df_training_dataset.copy()[target]\n\n\nQ1_edad = X['EDAD'].quantile(0.25)\nQ3_edad = X['EDAD'].quantile(0.75)\nIQR_edad = Q3_edad - Q1_edad\nX['EDADINF']=np.where(X['EDAD'] < (Q1_edad - 1.5 * IQR_edad),1,0)\nX['EDADSUP']=np.where(X['EDAD'] > (Q3_edad + 1.5 * IQR_edad),1,0)\nX.loc[(X['EDAD'] < (Q1_edad - 1.5 * IQR_edad)),['EDAD']]=(Q1_edad - 1.5 * IQR_edad)\nX.loc[(X['EDAD'] > (Q3_edad + 1.5 * IQR_edad)),['EDAD']]=(Q3_edad + 1.5 * IQR_edad)\n\nQ1_avg = X['AVG_DH'].quantile(0.25)\nQ3_avg = X['AVG_DH'].quantile(0.75)\nIQR_avg = Q3_avg - Q1_avg\nX['AVGINF']=np.where(X['AVG_DH'] < (Q1_avg - 1.5 * IQR_avg),1,0)\nX['AVGSUP']=np.where(X['AVG_DH'] > (Q3_avg + 1.5 * IQR_avg),1,0)\nX.loc[(X['AVG_DH'] < (Q1_avg - 1.5 * IQR_avg)),['AVG_DH']]=(Q1_avg - 1.5 * IQR_avg)\nX.loc[(X['AVG_DH'] > (Q3_avg + 1.5 * IQR_avg)),['AVG_DH']]=(Q3_avg + 1.5 * IQR_avg)\n\nQ1_min = X['MINUTES_DH'].quantile(0.25)\nQ3_min = X['MINUTES_DH'].quantile(0.75)\nIQR_min = Q3_min - Q1_min\n#X['MININF']=np.where(X['MINUTES_DH'] < (Q1_avg - 1.5 * IQR_avg),1,0)\n#X['MINSUP']=np.where(X['MINUTES_DH'] > (Q3_avg + 1.5 * IQR_avg),1,0)\nX.loc[(X['MINUTES_DH'] < (Q1_min - 1.5 * IQR_min)),['MINUTES_DH']]=(Q1_min - 1.5 * IQR_min)\nX.loc[(X['MINUTES_DH'] > (Q3_min + 1.5 * IQR_min)),['MINUTES_DH']]=(Q3_min + 1.5 * IQR_min)\n\n\nQ1_exp = X['EXPERIENCIA'].quantile(0.25)\nQ3_exp = X['EXPERIENCIA'].quantile(0.75)\nIQR_exp = Q3_exp - Q1_exp\nX.loc[(X['EXPERIENCIA'] < (Q1_exp - 1.5 * IQR_exp)),['EXPERIENCIA']]=(Q1_exp - 1.5 * IQR_exp)\nX.loc[(X['EXPERIENCIA'] > (Q3_exp + 1.5 * IQR_exp)),['EXPERIENCIA']]=(Q3_exp + 1.5 * IQR_exp)\n\n# #Detectando outliers\n# for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#     Q1 = X[i].quantile(0.25)\n#     Q3 = X[i].quantile(0.75)\n#     IQR = Q3 - Q1\n#     X.loc[(X[i] < (Q1 - 1.5 * IQR)),[i]]=(Q1 - 1.5 * IQR)\n#     #X.loc[(X[i] < (Q1 - 1.5 * IQR)),[i]]=0\n#     X.loc[(X[i] > (Q3 + 1.5* IQR)),[i]]=(Q3 + 1.5 * IQR)\n#     #X.loc[(X[i] > (Q3 + 1.5 * IQR)),[i]]=0\n\nX_imputar=X.copy()[['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA','GENERO_MASCULINO','GENERO_FEMENINO']]\nimp = IterativeImputer(max_iter=15)\nimp.fit(X_imputar)\nX_imputar=pd.DataFrame(imp.transform(X_imputar),columns=X_imputar.columns)\n\nfor i in list(X_imputar.columns):\n     X[i]=X_imputar[i]\n    \n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    X[i]=(X[i]-X[i].mean())/X[i].std()\n\n#for i in ['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA']:\n#    X[i]=(X[i]-X[i].min(axis=0))/(X[i].max(axis=0)-X[i].min(axis=0))\n\n#X.drop(labels=['ESTUDIO_PREV_MARKETING'],axis=1,inplace=True)\n\ngbreg = xgb.XGBRegressor(learning_rate=0.14,n_estimators=90,max_depth=3,alpha=0.9,n_jobs=-1)\ncv=cross_validate(gbreg,X,y,cv=15,scoring='r2')\nprint([cv['test_score'].mean()])\nprint([cv['test_score']])", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "--2020-09-18 06:36:45--  https://raw.githubusercontent.com/vanderlei-test/654986294958/master/train_dataset_digitalhouse.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 654532 (639K) [text/plain]\nSaving to: \u2018train_dataset_digitalhouse.csv.239\u2019\n\n100%[======================================>] 654,532     --.-K/s   in 0.02s   \n\n2020-09-18 06:36:45 (26.4 MB/s) - \u2018train_dataset_digitalhouse.csv.239\u2019 saved [654532/654532]\n\n[0.8100517042912898]\n[array([0.80802775, 0.81373226, 0.8404568 , 0.81872969, 0.8040137 ,\n       0.81576149, 0.80366462, 0.81095735, 0.80197893, 0.7865353 ,\n       0.82868156, 0.79031392, 0.81263414, 0.81941666, 0.79587139])]\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "X.shape", "execution_count": 5, "outputs": [{"output_type": "execute_result", "execution_count": 5, "data": {"text/plain": "(8995, 21)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "gbreg_final=xgb.XGBRegressor(learning_rate=0.14,n_estimators=90,max_depth=3,alpha=0.9,n_jobs=-1)\ngbreg_final.fit(X,y)", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "XGBRegressor(alpha=0.9, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.14, max_delta_step=0, max_depth=3,\n             min_child_weight=1, missing=nan, monotone_constraints='()',\n             n_estimators=90, n_jobs=-1, num_parallel_tree=1, random_state=0,\n             reg_alpha=0.899999976, reg_lambda=1, scale_pos_weight=1,\n             subsample=1, tree_method='exact', validate_parameters=1,\n             verbosity=None)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "X.MINUTES_DH", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "## Completar los datos necesarios para entregar la soluci\u00f3n\n\n### Como entrega de su soluci\u00f3n, esperamos los resultados num\u00e9ricos predichos por su modelo. Como entrada utilizar\u00e1 el archivo \"to_be_scored.csv\""}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/654986294958/master/to_be_scored_digitalhouse.csv\ndf_to_be_scored = pd.read_csv(r'to_be_scored_digitalhouse.csv')\ndf_to_be_scored.tail()", "execution_count": 7, "outputs": [{"output_type": "stream", "text": "--2020-09-18 06:36:56--  https://raw.githubusercontent.com/vanderlei-test/654986294958/master/to_be_scored_digitalhouse.csv\r\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\r\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 67308 (66K) [text/plain]\r\nSaving to: \u2018to_be_scored_digitalhouse.csv.10\u2019\r\n\r\n\r 0% [                                       ] 0           --.-K/s              \r100%[======================================>] 67,308      --.-K/s   in 0.004s  \r\n\r\n2020-09-18 06:36:56 (18.2 MB/s) - \u2018to_be_scored_digitalhouse.csv.10\u2019 saved [67308/67308]\r\n\r\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 7, "data": {"text/plain": "     Unnamed: 0  EDAD     GENERO RESIDENCIA    NV_ESTUDIO ESTUDIO_PREV  \\\n995         995  33.0  MASCULINO  ARGENTINA  UNIVERSITARY   COMMERCIAL   \n996         996  40.0  MASCULINO  ARGENTINA      TERTIARY   COMMERCIAL   \n997         997   NaN   FEMENINO  ARGENTINA  UNIVERSITARY  ENGINEERING   \n998         998   NaN  MASCULINO     MEXICO  UNIVERSITARY  ENGINEERING   \n999         999  36.0        NaN  ARGENTINA  UNIVERSITARY          NaN   \n\n         TRACK_DH  AVG_DH  MINUTES_DH  EXPERIENCIA  Unnamed: 10  \n995  PROGRAMACION     3.6      4576.5         15.1          NaN  \n996  PROGRAMACION     3.4      4542.9         26.1          NaN  \n997          DATA     3.4         NaN         27.4          NaN  \n998          DATA     3.7      4730.4          1.1          NaN  \n999  PROGRAMACION     3.3      4506.5         20.4          NaN  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>EDAD</th>\n      <th>GENERO</th>\n      <th>RESIDENCIA</th>\n      <th>NV_ESTUDIO</th>\n      <th>ESTUDIO_PREV</th>\n      <th>TRACK_DH</th>\n      <th>AVG_DH</th>\n      <th>MINUTES_DH</th>\n      <th>EXPERIENCIA</th>\n      <th>Unnamed: 10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>995</th>\n      <td>995</td>\n      <td>33.0</td>\n      <td>MASCULINO</td>\n      <td>ARGENTINA</td>\n      <td>UNIVERSITARY</td>\n      <td>COMMERCIAL</td>\n      <td>PROGRAMACION</td>\n      <td>3.6</td>\n      <td>4576.5</td>\n      <td>15.1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>996</td>\n      <td>40.0</td>\n      <td>MASCULINO</td>\n      <td>ARGENTINA</td>\n      <td>TERTIARY</td>\n      <td>COMMERCIAL</td>\n      <td>PROGRAMACION</td>\n      <td>3.4</td>\n      <td>4542.9</td>\n      <td>26.1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>997</td>\n      <td>NaN</td>\n      <td>FEMENINO</td>\n      <td>ARGENTINA</td>\n      <td>UNIVERSITARY</td>\n      <td>ENGINEERING</td>\n      <td>DATA</td>\n      <td>3.4</td>\n      <td>NaN</td>\n      <td>27.4</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>998</td>\n      <td>NaN</td>\n      <td>MASCULINO</td>\n      <td>MEXICO</td>\n      <td>UNIVERSITARY</td>\n      <td>ENGINEERING</td>\n      <td>DATA</td>\n      <td>3.7</td>\n      <td>4730.4</td>\n      <td>1.1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>999</td>\n      <td>36.0</td>\n      <td>NaN</td>\n      <td>ARGENTINA</td>\n      <td>UNIVERSITARY</td>\n      <td>NaN</td>\n      <td>PROGRAMACION</td>\n      <td>3.3</td>\n      <td>4506.5</td>\n      <td>20.4</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "df_to_be_scored.isnull().sum()", "execution_count": 8, "outputs": [{"output_type": "execute_result", "execution_count": 8, "data": {"text/plain": "Unnamed: 0         0\nEDAD             139\nGENERO           132\nRESIDENCIA       169\nNV_ESTUDIO       142\nESTUDIO_PREV     140\nTRACK_DH         153\nAVG_DH           160\nMINUTES_DH       164\nEXPERIENCIA      145\nUnnamed: 10     1000\ndtype: int64"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# \u00a1Atenci\u00f3n!\n\n### El marco de datos ``to_be_scored`` es su \"hoja de evaluaci\u00f3n\". Tenga en cuenta que la columna \"target\" no existe en esta muestra, por lo que no se puede utilizar para modelos de entrenamiento basados en el aprendizaje supervisado."}, {"metadata": {}, "cell_type": "markdown", "source": "# \u00a1Atenci\u00f3n!\n\n### Debes realizar los mismos pasos de procesamiento previo que hiciste en el conjunto de datos de entrenamiento antes de calificar la \"hoja de respuestas\""}, {"metadata": {}, "cell_type": "code", "source": "df_to_be_scored=pd.get_dummies(df_to_be_scored, columns=['GENERO','RESIDENCIA','NV_ESTUDIO','ESTUDIO_PREV','TRACK_DH'])", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "features_scored=['EDAD', \n          'AVG_DH',\n          'MINUTES_DH', \n          'EXPERIENCIA',\n           'GENERO_FEMENINO', \n          'GENERO_MASCULINO', \n            'RESIDENCIA_ARGENTINA','RESIDENCIA_BRAZIL', 'RESIDENCIA_MEXICO', \n            'NV_ESTUDIO_POST_GRADUATE','NV_ESTUDIO_TERTIARY', 'NV_ESTUDIO_UNIVERSITARY',\n            'ESTUDIO_PREV_BUSINESS', 'ESTUDIO_PREV_COMMERCIAL',\n           'ESTUDIO_PREV_DEVELOPMENT', 'ESTUDIO_PREV_ENGINEERING',\n           'ESTUDIO_PREV_MARKETING']\nX_scored=df_to_be_scored.copy()[features_scored]\n\nX_scored['EDADINF']=np.where(X_scored['EDAD'] < (Q1_edad - 1.5 * IQR_edad),1,0)\nX_scored['EDADSUP']=np.where(X_scored['EDAD'] > (Q3_edad + 1.5 * IQR_edad),1,0)\nX_scored.loc[(X_scored['EDAD'] < (Q1_edad - 1.5 * IQR_edad)),['EDAD']]=(Q1_edad - 1.5 * IQR_edad)\nX_scored.loc[(X_scored['EDAD'] > (Q3_edad + 1.5 * IQR_edad)),['EDAD']]=(Q3_edad + 1.5 * IQR_edad)\n\nX_scored['AVGINF']=np.where(X_scored['AVG_DH'] < (Q1_avg - 1.5 * IQR_avg),1,0)\nX_scored['AVGSUP']=np.where(X_scored['AVG_DH'] > (Q3_avg + 1.5 * IQR_avg),1,0)\nX_scored.loc[(X_scored['AVG_DH'] < (Q1_avg - 1.5 * IQR_avg)),['AVG_DH']]=(Q1_avg - 1.5 * IQR_avg)\nX_scored.loc[(X_scored['AVG_DH'] > (Q3_avg + 1.5 * IQR_avg)),['AVG_DH']]=(Q3_avg + 1.5 * IQR_avg)\n\nX_scored.loc[(X_scored['MINUTES_DH'] < (Q1_min - 1.5 * IQR_min)),['MINUTES_DH']]=(Q1_min - 1.5 * IQR_min)\nX_scored.loc[(X_scored['MINUTES_DH'] > (Q3_min + 1.5 * IQR_min)),['MINUTES_DH']]=(Q3_min + 1.5 * IQR_min)\n\nX_scored.loc[(X_scored['EXPERIENCIA'] < (Q1_exp - 1.5 * IQR_exp)),['EXPERIENCIA']]=(Q1_exp - 1.5 * IQR_exp)\nX_scored.loc[(X_scored['EXPERIENCIA'] > (Q3_exp + 1.5 * IQR_exp)),['EXPERIENCIA']]=(Q3_exp + 1.5 * IQR_exp)", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X_imputar_scored=X_scored.copy()[['EDAD','AVG_DH','MINUTES_DH','EXPERIENCIA','GENERO_MASCULINO','GENERO_FEMENINO']]\nX_imputar_scored=pd.DataFrame(imp.transform(X_imputar_scored),columns=X_imputar_scored.columns)\nfor i in list(X_imputar_scored.columns):\n    X_scored[i]=X_imputar_scored[i]", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>\n\n### Hacer las predicciones con el m\u00e9todo \"predict()\" de sklearn y agregar los resultados en el marco de datos de la \"hoja de evaluaci\u00f3n\""}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "y_pred = gbreg_final.predict(X_scored)\nX_scored['target'] = y_pred\nX_scored.tail()", "execution_count": 12, "outputs": [{"output_type": "execute_result", "execution_count": 12, "data": {"text/plain": "          EDAD  AVG_DH   MINUTES_DH  EXPERIENCIA  GENERO_FEMENINO  \\\n995  33.000000     3.6  4576.500000         15.1              0.0   \n996  40.000000     3.4  4542.900000         26.1              0.0   \n997  41.765904     3.4  4403.877191         27.4              1.0   \n998  23.620997     3.7  4730.400000          1.1              0.0   \n999  36.000000     3.3  4506.500000         20.4              0.0   \n\n     GENERO_MASCULINO  RESIDENCIA_ARGENTINA  RESIDENCIA_BRAZIL  \\\n995               1.0                     1                  0   \n996               1.0                     1                  0   \n997               0.0                     1                  0   \n998               1.0                     0                  0   \n999               0.0                     1                  0   \n\n     RESIDENCIA_MEXICO  NV_ESTUDIO_POST_GRADUATE  ...  ESTUDIO_PREV_BUSINESS  \\\n995                  0                         0  ...                      0   \n996                  0                         0  ...                      0   \n997                  0                         0  ...                      0   \n998                  1                         0  ...                      0   \n999                  0                         0  ...                      0   \n\n     ESTUDIO_PREV_COMMERCIAL  ESTUDIO_PREV_DEVELOPMENT  \\\n995                        1                         0   \n996                        1                         0   \n997                        0                         0   \n998                        0                         0   \n999                        0                         0   \n\n     ESTUDIO_PREV_ENGINEERING  ESTUDIO_PREV_MARKETING  EDADINF  EDADSUP  \\\n995                         0                       0        0        0   \n996                         0                       0        0        0   \n997                         1                       0        0        0   \n998                         1                       0        0        0   \n999                         0                       0        0        0   \n\n     AVGINF  AVGSUP      target  \n995       0       0   86.002945  \n996       0       0   93.251213  \n997       0       0  101.077599  \n998       0       0   83.133133  \n999       0       0   91.182457  \n\n[5 rows x 22 columns]", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EDAD</th>\n      <th>AVG_DH</th>\n      <th>MINUTES_DH</th>\n      <th>EXPERIENCIA</th>\n      <th>GENERO_FEMENINO</th>\n      <th>GENERO_MASCULINO</th>\n      <th>RESIDENCIA_ARGENTINA</th>\n      <th>RESIDENCIA_BRAZIL</th>\n      <th>RESIDENCIA_MEXICO</th>\n      <th>NV_ESTUDIO_POST_GRADUATE</th>\n      <th>...</th>\n      <th>ESTUDIO_PREV_BUSINESS</th>\n      <th>ESTUDIO_PREV_COMMERCIAL</th>\n      <th>ESTUDIO_PREV_DEVELOPMENT</th>\n      <th>ESTUDIO_PREV_ENGINEERING</th>\n      <th>ESTUDIO_PREV_MARKETING</th>\n      <th>EDADINF</th>\n      <th>EDADSUP</th>\n      <th>AVGINF</th>\n      <th>AVGSUP</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>995</th>\n      <td>33.000000</td>\n      <td>3.6</td>\n      <td>4576.500000</td>\n      <td>15.1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>86.002945</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>40.000000</td>\n      <td>3.4</td>\n      <td>4542.900000</td>\n      <td>26.1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>93.251213</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>41.765904</td>\n      <td>3.4</td>\n      <td>4403.877191</td>\n      <td>27.4</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>101.077599</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>23.620997</td>\n      <td>3.7</td>\n      <td>4730.400000</td>\n      <td>1.1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>83.133133</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>36.000000</td>\n      <td>3.3</td>\n      <td>4506.500000</td>\n      <td>20.4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>91.182457</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 22 columns</p>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "X_scored.shape", "execution_count": 13, "outputs": [{"output_type": "execute_result", "execution_count": 13, "data": {"text/plain": "(1000, 22)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# \u00a1Atenci\u00f3n!\n\n### La columna agregada con los resultados debe llamarse \"target\", de lo contrario, su env\u00edo fallar\u00e1."}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>\n\n### Exportar el marco de datos de resultados como un archivo .csv a su proyecto de Watson Studio."}, {"metadata": {}, "cell_type": "code", "source": "project.save_data(file_name=\"results.csv\", data=X_scored.to_csv(index=False))", "execution_count": 14, "outputs": [{"output_type": "execute_result", "execution_count": 14, "data": {"text/plain": "{'file_name': 'results.csv',\n 'message': 'File saved to project storage.',\n 'bucket_name': 'ultimo-donotdelete-pr-zrfq3fvphc7tzf',\n 'asset_id': 'c5e0e950-14b4-427c-a154-491209a9f345'}"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}